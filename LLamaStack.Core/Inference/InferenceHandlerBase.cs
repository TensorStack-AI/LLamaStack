using LLama;
using LLama.Abstractions;
using LLama.Common;
using LLamaStack.Core.Common;
using System.Runtime.CompilerServices;

namespace LLamaStack.Core.Inference
{
    //using llama_token = Int32;

    public abstract class InferenceHandlerBase : IInferenceHandler
    {
        /// <summary>
        /// The tokens that were already processed by the model.
        /// </summary>
        protected int _pastTokensCount; // n_past

        /// <summary>
        /// The tokens that were consumed by the model during the current inference.
        /// </summary>
        protected int _consumedTokensCount; // n_consume

        /// <summary>
        /// A container for the tokens of input.
        /// </summary>
        protected List<TokenData> _promptTokens = new();

        /// <summary>
        /// A container of the tokens to be processed and after processed.
        /// </summary>
        protected List<TokenData> _currentTokens = new();

        /// <summary>
        /// The last tokens generated by the model.
        /// </summary>
        protected FixedSizeQueue<TokenData> _lastTokens;

        /// <summary>
        /// Current "mu" value for mirostat sampling
        /// </summary>
        protected float? MirostatMu { get; set; }



        protected InferenceHandlerBase(LLamaContext context)
        {
            Context = context;
            _pastTokensCount = 0;
            _consumedTokensCount = 0;
            _lastTokens = new FixedSizeQueue<TokenData>(Context.ContextSize).FillWith(new(0));
        }

        /// <summary>
        /// The context used by the handler.
        /// </summary>
        public LLamaContext Context { get; }

        public abstract InferenceType Type { get; }

        public virtual Task<InferenceHandlerState> GetState()
        {
            var state = new InferenceHandlerState()
            {
                EmbedInps = _promptTokens,
                ConsumedTokensCount = _consumedTokensCount,
                Embeds = _currentTokens,
                LastTokens = _lastTokens.ToArray(),
                PastTokensCount = _pastTokensCount,
                LastTokensCapacity = _lastTokens.Capacity,
                MirostatMu = MirostatMu
            };
            return Task.FromResult(state);
        }

        /// <inheritdoc />
        public virtual Task SetState(InferenceHandlerState state)
        {
            ArgumentNullException.ThrowIfNull(state);

            _currentTokens = state.Embeds;
            MirostatMu = state.MirostatMu;
            _promptTokens = state.EmbedInps;
            _pastTokensCount = state.PastTokensCount;
            _consumedTokensCount = state.ConsumedTokensCount;
            _lastTokens = new FixedSizeQueue<TokenData>(state.LastTokensCapacity, state.LastTokens);
            return Task.CompletedTask;
        }

        /// <summary>
        /// After running out of the context, take some tokens from the original prompt and recompute the logits in batches.
        /// </summary>
        /// <param name="tokensToKeep"></param>
        protected virtual Task HandleRunOutOfContext(int tokensToKeep)
        {
            // if we run out of context:
            // - take the tokensToKeep first tokens from the original prompt (via n_past)
            // - take half of the last (n_ctx - tokensToKeep) tokens and recompute the logits in batches
            int n_left = _pastTokensCount - tokensToKeep;

            _pastTokensCount = Math.Max(1, tokensToKeep);

            // insert n_left/2 tokens at the start of embed from last_n_tokens
            _currentTokens.InsertRange(0, _lastTokens.Take(_lastTokens.Count - _currentTokens.Count).Skip(Context.ContextSize - n_left / 2 - _currentTokens.Count));

            return Task.CompletedTask;
        }

        /// <summary>
        /// Decide whether to continue the loop.
        /// </summary>
        /// <param name="args"></param>
        /// <returns></returns>
        protected abstract Task<bool> GetLoopCondition(InferStateArgs args);

        /// <summary>
        /// Preprocess the inputs before the inference.
        /// </summary>
        /// <param name="text"></param>
        /// <param name="args"></param>
        protected abstract Task PreprocessInputs(string text, InferStateArgs args);

        /// <summary>
        /// Do some post processing after the inference.
        /// </summary>
        /// <param name="inferenceParams"></param>
        /// <param name="args"></param>
        /// <param name="extraOutputs"></param>
        /// <returns></returns>
        protected abstract Task<bool> PostProcess(IInferenceParams inferenceParams, InferStateArgs args);

        /// <summary>
        /// The core inference logic.
        /// </summary>
        /// <param name="inferenceParams"></param>
        /// <param name="args"></param>
        protected abstract Task InferInternal(IInferenceParams inferenceParams, InferStateArgs args);


        /// <summary>
        /// Execute the inference.
        /// </summary>
        /// <param name="text"></param>
        /// <param name="inferenceParams"></param>
        /// <param name="cancellationToken"></param>
        /// <returns></returns>
        public async virtual IAsyncEnumerable<TokenData> InferAsync(string text, IInferenceParams? inferenceParams = null, [EnumeratorCancellation] CancellationToken cancellationToken = default)
        {
            cancellationToken.ThrowIfCancellationRequested();
            inferenceParams ??= new InferenceParams();

            InferStateArgs args = new InferStateArgs()
            {
                Antiprompts = inferenceParams.AntiPrompts.ToList(),
                RemainedTokens = inferenceParams.MaxTokens,
                ReturnValue = false,
                WaitForInput = false
            };

            await PreprocessInputs(text, args);

            while (await GetLoopCondition(args))
            {
                if (cancellationToken.IsCancellationRequested)
                {
                    break;
                }

                await InferInternal(inferenceParams, args);

                if (args.ReturnValue)
                {
                    foreach (var embed in _currentTokens)
                        yield return embed;
                }

                var breakGeneration = await PostProcess(inferenceParams, args);
                if (breakGeneration)
                {
                    break;
                }
            }
        }





        /// <summary>
        /// State arguments that are used in single inference
        /// </summary>
        protected class InferStateArgs
        {
            public IList<string> Antiprompts { get; set; }

            /// <summary>
            /// Tokens count remained to be used. (n_remain)
            /// </summary>
            public int RemainedTokens { get; set; }

            public bool ReturnValue { get; set; }

            public bool WaitForInput { get; set; }
        }


    }
}
