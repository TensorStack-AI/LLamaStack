using LLama.Abstractions;
using LLama.Common;
using LLamaStack.Core.Common;
using LLamaStack.Core.Services;
using System.Runtime.CompilerServices;

namespace LLamaStack.Core.Inference
{
    public abstract class InferenceHandlerBase<T> : IInferenceHandler
    {
        /// <summary>
        /// The tokens that were already processed by the model.
        /// </summary>
        protected int _pastTokensCount; // n_past

        /// <summary>
        /// The tokens that were consumed by the model during the current inference.
        /// </summary>
        protected int _consumedTokensCount; // n_consume

        /// <summary>
        /// A container for the tokens of input.
        /// </summary>
        protected List<TokenData> _promptTokens = new();

        /// <summary>
        /// A container of the tokens to be processed and after processed.
        /// </summary>
        protected List<TokenData> _currentTokens = new();

        /// <summary>
        /// The last tokens generated by the model.
        /// </summary>
        protected FixedSizeQueue<TokenData> _lastTokens;

        /// <summary>
        /// The model used by the handler.
        /// </summary>
        protected LLamaStackModel<T> _model;

        /// <summary>
        /// The context used by the handler.
        /// </summary>
        protected LLamaStackContext _context;

        /// <summary>
        /// The SampleService used for token selection
        /// </summary>
        protected ISampleService _sampleService;


        /// <summary>
        /// Initializes a new instance of the <see cref="InferenceHandlerBase{T}"/> class.
        /// </summary>
        /// <param name="model">The model.</param>
        /// <param name="context">The context.</param>
        protected InferenceHandlerBase(LLamaStackModel<T> model, LLamaStackContext context)
        {
            _model = model;
            _context = context;
            _pastTokensCount = 0;
            _consumedTokensCount = 0;
            _sampleService = new SampleService(_context);
            _lastTokens = new FixedSizeQueue<TokenData>(_context.ContextSize).FillWith(new(0));
        }


        /// <summary>
        /// Gets the InferenceType.
        /// </summary>
        public abstract InferenceType Type { get; }


        /// <summary>
        /// After running out of the context, take some tokens from the original prompt and recompute the logits in batches.
        /// </summary>
        /// <param name="tokensToKeep"></param>
        protected virtual Task HandleRunOutOfContext(int tokensToKeep)
        {
            // if we run out of context:
            // - take the tokensToKeep first tokens from the original prompt (via n_past)
            // - take half of the last (n_ctx - tokensToKeep) tokens and recompute the logits in batches
            int n_left = _pastTokensCount - tokensToKeep;

            _pastTokensCount = Math.Max(1, tokensToKeep);

            // insert n_left/2 tokens at the start of embed from last_n_tokens
            _currentTokens.InsertRange(0, _lastTokens.Take(_lastTokens.Count - _currentTokens.Count).Skip(_context.ContextSize - n_left / 2 - _currentTokens.Count));

            return Task.CompletedTask;
        }


        /// <summary>
        /// Decide whether to continue the loop.
        /// </summary>
        /// <param name="args"></param>
        /// <returns></returns>
        protected abstract Task<bool> GetLoopCondition(InferStateArgs args);


        /// <summary>
        /// Preprocess the inputs before the inference.
        /// </summary>
        /// <param name="text"></param>
        /// <param name="args"></param>
        protected abstract Task PreprocessInputs(string text, InferStateArgs args);


        /// <summary>
        /// Do some post processing after the inference.
        /// </summary>
        /// <param name="inferenceParams"></param>
        /// <param name="args"></param>
        /// <param name="extraOutputs"></param>
        /// <returns></returns>
        protected abstract Task<bool> PostProcess(IInferenceParams inferenceParams, InferStateArgs args);


        /// <summary>
        /// The core inference logic.
        /// </summary>
        /// <param name="inferenceParams"></param>
        /// <param name="args"></param>
        protected abstract Task InferInternal(IInferenceParams inferenceParams, InferStateArgs args);


        /// <summary>
        /// Execute the inference.
        /// </summary>
        /// <param name="text"></param>
        /// <param name="inferenceParams"></param>
        /// <param name="cancellationToken"></param>
        /// <returns></returns>
        public async virtual IAsyncEnumerable<TokenData> InferAsync(string text, IInferenceParams inferenceParams = null, [EnumeratorCancellation] CancellationToken cancellationToken = default)
        {
            cancellationToken.ThrowIfCancellationRequested();
            inferenceParams ??= new InferenceParams();

            InferStateArgs args = new InferStateArgs()
            {
                Antiprompts = inferenceParams.AntiPrompts.ToList(),
                RemainedTokens = inferenceParams.MaxTokens,
                ReturnValue = false,
                WaitForInput = false
            };

            await PreprocessInputs(text, args);

            while (await GetLoopCondition(args))
            {
                if (cancellationToken.IsCancellationRequested)
                {
                    break;
                }

                await InferInternal(inferenceParams, args);

                if (args.ReturnValue)
                {
                    foreach (var embed in _currentTokens)
                        yield return embed;
                }

                var breakGeneration = await PostProcess(inferenceParams, args);
                if (breakGeneration)
                {
                    break;
                }
            }
        }


        /// <summary>
        /// Gets the state.
        /// </summary>
        /// <returns></returns>
        public virtual Task<InferenceHandlerState> GetStateAsync()
        {
            var state = new InferenceHandlerState()
            {
                EmbedInps = _promptTokens,
                ConsumedTokensCount = _consumedTokensCount,
                Embeds = _currentTokens,
                LastTokens = _lastTokens.ToArray(),
                PastTokensCount = _pastTokensCount,
                LastTokensCapacity = _lastTokens.Capacity,
                MirostatMu = _sampleService.MirostatMu
            };
            return Task.FromResult(state);
        }


        /// <summary>
        /// Sets the state.
        /// </summary>
        /// <param name="state">The state.</param>
        /// <returns></returns>
        /// <exception cref="ArgumentNullException"></exception>
        public virtual Task SetStateAsync(InferenceHandlerState state)
        {
            ArgumentNullException.ThrowIfNull(state);

            _currentTokens = state.Embeds;
            _promptTokens = state.EmbedInps;
            _pastTokensCount = state.PastTokensCount;
            _consumedTokensCount = state.ConsumedTokensCount;
            _lastTokens = new FixedSizeQueue<TokenData>(state.LastTokensCapacity, state.LastTokens);
            _sampleService.MirostatMu = state.MirostatMu;
            return Task.CompletedTask;
        }


        /// <summary>
        /// State arguments that are used in single inference
        /// </summary>
        protected sealed class InferStateArgs
        {
            /// <summary>
            /// Gets or sets the antiprompts.
            /// </summary>
            public IList<string> Antiprompts { get; set; }

            /// <summary>
            /// Tokens count remained to be used. (n_remain)
            /// </summary>
            public int RemainedTokens { get; set; }

            /// <summary>
            /// Gets or sets a value indicating whether return value.
            /// </summary>
            public bool ReturnValue { get; set; }

            /// <summary>
            /// Gets or sets a value indicating whether to wait for input.
            /// </summary>
            public bool WaitForInput { get; set; }
        }
    }
}
